{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAin aim is to understand more about the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "## Display all the columns of the dataframe\n",
    "pd.pandas.set_option('display.max_columns',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elasticnet\n",
    "elasticnet_alphas = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "elasticnet_l1ratios = [0.8, 0.85, 0.9, 0.95, 1]\n",
    "#lasso\n",
    "lasso_alphas = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "#ridge\n",
    "ridge_alphas = [13.5, 14, 14.5, 15, 15.5]\n",
    "\n",
    "\n",
    "MODELS = { \n",
    "    \"elasticnet\" : make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=elasticnet_alphas, l1_ratio=elasticnet_l1ratios)),\n",
    "     \"lasso\" : make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=lasso_alphas, random_state=42)),\n",
    "     \"ridge\" : make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas)),\n",
    "     \"gradb\" : GradientBoostingRegressor(n_estimators=6000, learning_rate=0.01,\n",
    "                                  max_depth=4, max_features='sqrt',\n",
    "                                  min_samples_leaf=15, min_samples_split=10,\n",
    "                                  loss='huber', random_state=42),\n",
    "\n",
    "    \"svr\" : make_pipeline(RobustScaler(),\n",
    "                    SVR(C=20, epsilon=0.008, gamma=0.0003)),\n",
    "\n",
    "    \"xgboost\" : XGBRegressor(learning_rate=0.01, n_estimators=6000,\n",
    "                       max_depth=3, min_child_weight=0,\n",
    "                       gamma=0, subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:squarederror', nthread=-1,\n",
    "                       scale_pos_weight=1, seed=27,\n",
    "                       reg_alpha=0.00006, random_state=42)}\n",
    "\n",
    "MODELS_stack = StackingCVRegressor(regressors=(MODELS['elasticnet'], MODELS['gradb'], MODELS['lasso'], \n",
    "                                          MODELS['ridge'], MODELS['svr'], MODELS['xgboost']),\n",
    "                              meta_regressor=MODELS['xgboost'],\n",
    "                              use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    #lib\n",
    "\n",
    "    temporal_features = [feature for feature in df.columns if 'Yr' in feature or 'Year' in feature or 'Mo' in feature]\n",
    "    numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O' and feature not in temporal_features and feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n",
    "    categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O' and feature not in temporal_features]\n",
    "\n",
    "    \n",
    "    #feature-eng on temporal-dataset\n",
    "\n",
    "    for feature in temporal_features:\n",
    "        if feature == 'YrSold' or feature == 'MoSold':\n",
    "            pass\n",
    "        else:\n",
    "            df[feature] = df['YrSold'] - df[feature]\n",
    "\n",
    "    df['YrSold'] = df['YrSold'].astype(str)\n",
    "    df['MoSold'] = df['MoSold'].astype(str) \n",
    "    df['MSSubClass'] = df['MSSubClass'].apply(str)\n",
    "    \n",
    "    \n",
    "    #fill-na\n",
    "\n",
    "    for feature in numeric_features:\n",
    "        df[feature] = df.groupby(\"Neighborhood\")[feature].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        df[feature] = df[feature].fillna(\"Missing\")\n",
    "\n",
    "    for feature in temporal_features:\n",
    "        if feature == 'YrSold' or feature == 'MoSold':\n",
    "            df[feature] = df[feature].fillna(\"Missing\")\n",
    "        else:\n",
    "            df[feature] = df[feature].fillna(0)\n",
    "\n",
    "    #feature-generation\n",
    "\n",
    "    df['TotalHouseSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "    df['TotalLot'] = df['LotFrontage'] + df['LotArea']\n",
    "\n",
    "    df['TotalBsmtFin'] = df['BsmtFinSF1'] + df['BsmtFinSF2']\n",
    "    \n",
    "    df['TotalBath'] = df['FullBath'] + df['HalfBath']\n",
    "\n",
    "    df['TotalPorch'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch']\n",
    "\n",
    "    #feature-selection (multi-correnality)\n",
    "\n",
    "    #df.drop(['TotalBsmtFin','LotArea','TotalBsmtSF','GrLivArea','GarageYrBlt','GarageArea'],axis=1,inplace=True)\n",
    "\n",
    "    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'YrSold', 'MoSold')\n",
    "    # process columns, apply LabelEncoder to categorical features\n",
    "    for c in cols:\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(df[c].values)) \n",
    "        df[c] = lbl.transform(list(df[c].values))\n",
    "\n",
    "    #some more-feature engineering:\n",
    "\n",
    "    df[\"TotalGarageQual\"] = df[\"GarageQual\"] * df[\"GarageCond\"]\n",
    "    df[\"TotalExteriorQual\"] = df[\"ExterQual\"] * df[\"ExterCond\"]\n",
    "    \n",
    "\n",
    "    #df.drop([\"PoolQC\"],axis=1,inplace=True)\n",
    "\n",
    "    # box_cox\n",
    "\n",
    "    numeric_feats = [feature for feature in df.columns if df[feature].dtype != \"object\" and feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    \n",
    "    skewness = skewness[abs(skewness) > 0.75]\n",
    "    \n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        df[feat] = boxcox1p(df[feat], lam)\n",
    "\n",
    "\n",
    "    #rare features \n",
    "    features = [feature for feature in df.columns if df[feature].dtype == 'O']   \n",
    "\n",
    "    for feature in features:\n",
    "        abc = df[feature].value_counts().to_dict()\n",
    "        for key, value in abc.items():\n",
    "            if value/len(df[feature]) < 0.01:\n",
    "                df.loc[:,feature][df[feature]==key]=\"rare\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold,model):\n",
    "\n",
    "    #loading\n",
    "    df = pd.read_csv('/home/hazim/Desktop/Advanced-house-price-prediction/input/train.csv')\n",
    "    df_test = pd.read_csv('/home/hazim/Desktop/Advanced-house-price-prediction/input/test.csv')\n",
    "\n",
    "    #K-foldding\n",
    "    \n",
    "    df[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    for f, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.SalePrice.values)):\n",
    "        print(len(train_idx), len(val_idx))\n",
    "        df.loc[val_idx, 'kfold'] = f\n",
    "\n",
    "    #function run\n",
    "    df = feature_engineering(df)\n",
    "    df_test = feature_engineering(df_test)  \n",
    "\n",
    "     #Some missing values still-were comming!! (please fix it in the above feature-eng(function) otherwise this also works fine)\n",
    "    df_test.fillna(0,inplace=True) \n",
    "\n",
    "    df.SalePrice = np.log1p(df.SalePrice)\n",
    "\n",
    "    #concat\n",
    "    df = pd.concat([df,df_test],axis=0)\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #Feature_selection\n",
    "\n",
    "    for feature in df.columns:\n",
    "        all_value_counts = df[feature].value_counts()\n",
    "        zero_value_counts = all_value_counts.iloc[0]\n",
    "        if zero_value_counts / len(df) > 0.99:\n",
    "            df.drop(feature,axis=1,inplace=True)\n",
    "\n",
    "    #split back\n",
    "    \n",
    "    df_test = df.loc[df[\"Id\"].between(1461,2919)]\n",
    "\n",
    "    df =  df.loc[df[\"Id\"].between(1,1460)]\n",
    "    \n",
    "\n",
    "    \n",
    "    #train-tests-split\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    numeric_features = [feature for feature in df_train.columns if feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n",
    "    \n",
    "    x_train = df_train[numeric_features].values\n",
    "    x_valid = df_valid[numeric_features].values\n",
    "\n",
    "    \n",
    "    #regressor models\n",
    "\n",
    "    reg = MODELS[model]\n",
    "\n",
    "    reg.fit(x_train,df_train.SalePrice.values)\n",
    "    valid_preds = reg.predict(x_valid)\n",
    "    test_preds = reg.predict(df_test[numeric_features].values)\n",
    "    \n",
    "    \n",
    "    #joblib.dump(features, os.path.join(config.models_location, f\"{model}_{fold}_columns.bin\"))\n",
    "\n",
    "    # scoring\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(df_valid.SalePrice.values, valid_preds))\n",
    "    mae = mean_absolute_error(df_valid.SalePrice.values, valid_preds)\n",
    "    \n",
    "    print(f\"FOLD={fold}, MODEL = {model}, RMSE = {rmse}, MAE ={mae} \")\n",
    "    return(test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOLD=0, MODEL = elasticnet, RMSE = 0.14725744693691908, MAE =0.08390364493367412 \n",
      "FOLD=0, MODEL = lasso, RMSE = 0.14793166888193054, MAE =0.08436734511887815 \n",
      "FOLD=0, MODEL = ridge, RMSE = 0.1475926769008778, MAE =0.08417537938578107 \n",
      "FOLD=0, MODEL = gradb, RMSE = 0.13762515330268787, MAE =0.08245005123861744 \n",
      "FOLD=0, MODEL = svr, RMSE = 0.1539241032735953, MAE =0.08459413195601814 \n",
      "FOLD=0, MODEL = xgboost, RMSE = 0.13923803706916, MAE =0.08674371715519258 \n",
      "FOLD=1, MODEL = elasticnet, RMSE = 0.11335730173230714, MAE =0.07398889316711908 \n",
      "FOLD=1, MODEL = lasso, RMSE = 0.11335730173230714, MAE =0.07398889316711908 \n",
      "FOLD=1, MODEL = ridge, RMSE = 0.11342953121900298, MAE =0.07429427480447665 \n",
      "FOLD=1, MODEL = gradb, RMSE = 0.10914808622096173, MAE =0.06996841602700234 \n",
      "FOLD=1, MODEL = svr, RMSE = 0.10693056388911608, MAE =0.06791104658510982 \n",
      "FOLD=1, MODEL = xgboost, RMSE = 0.11004067929134877, MAE =0.07070801957679702 \n",
      "FOLD=2, MODEL = elasticnet, RMSE = 0.1240207562343852, MAE =0.08663347942402184 \n",
      "FOLD=2, MODEL = lasso, RMSE = 0.12400988015750031, MAE =0.08666184360568917 \n",
      "FOLD=2, MODEL = ridge, RMSE = 0.12651338743038246, MAE =0.08874590948081561 \n",
      "FOLD=2, MODEL = gradb, RMSE = 0.12464595220522222, MAE =0.08283488718473091 \n",
      "FOLD=2, MODEL = svr, RMSE = 0.12029282816259589, MAE =0.08237026070266804 \n",
      "FOLD=2, MODEL = xgboost, RMSE = 0.1267955399015471, MAE =0.08395792914418099 \n"
     ]
    }
   ],
   "source": [
    "preds_df = pd.DataFrame()\n",
    "\n",
    "for fold in range(3):\n",
    "    for keys,items in MODELS.items():    \n",
    "        preds_df[\"fold\"+str(fold)+keys] = run(fold,keys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1095 365\n1095 365\n1095 365\n1095 365\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d053aa7db3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mpreds_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fold\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"stack\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d053aa7db3bb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold, model)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODELS_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSalePrice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/mlxtend/regressor/stacking_cv_regression.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 fit_params=fit_params, pre_dispatch=self.pre_dispatch)\n\u001b[0;32m--> 191\u001b[0;31m                     for regr in self.regr_])\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# save meta-features for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/mlxtend/regressor/stacking_cv_regression.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 fit_params=fit_params, pre_dispatch=self.pre_dispatch)\n\u001b[0;32m--> 191\u001b[0;31m                     for regr in self.regr_])\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# save meta-features for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    771\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    772\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 773\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    498\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m    555\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 212\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#if you want to run,stacked regressor also. (it takes time, but results are good)\n",
    "\n",
    "def run(fold,model):\n",
    "\n",
    "    #loading\n",
    "    df = pd.read_csv('/home/hazim/Desktop/Advanced-house-price-prediction/input/train.csv')\n",
    "    df_test = pd.read_csv('/home/hazim/Desktop/Advanced-house-price-prediction/input/test.csv')\n",
    "\n",
    "    #K-foldding\n",
    "    \n",
    "    df[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    for f, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.SalePrice.values)):\n",
    "        print(len(train_idx), len(val_idx))\n",
    "        df.loc[val_idx, 'kfold'] = f\n",
    "\n",
    "    #function run\n",
    "    df = feature_engineering(df)\n",
    "    df_test = feature_engineering(df_test)  \n",
    "\n",
    "    #Some missing values still-were comming!! (please fix it in the above feature-eng(function) otherwise this also works fine)\n",
    "    df_test.fillna(0,inplace=True) \n",
    "\n",
    "    df.SalePrice = np.log1p(df.SalePrice)\n",
    "\n",
    "    #concat\n",
    "    df = pd.concat([df,df_test],axis=0)\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #Feature_selection\n",
    "\n",
    "    for feature in df.columns:\n",
    "        all_value_counts = df[feature].value_counts()\n",
    "        zero_value_counts = all_value_counts.iloc[0]\n",
    "        if zero_value_counts / len(df) > 0.99:\n",
    "            df.drop(feature,axis=1,inplace=True)\n",
    "\n",
    "    #split back\n",
    "    \n",
    "    df_test = df.loc[df[\"Id\"].between(1461,2919)]\n",
    "\n",
    "    df =  df.loc[df[\"Id\"].between(1,1460)]\n",
    "    \n",
    "\n",
    "    \n",
    "    #train-tests-split\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    numeric_features = [feature for feature in df_train.columns if feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n",
    "    \n",
    "    x_train = df_train[numeric_features].values\n",
    "    x_valid = df_valid[numeric_features].values\n",
    "\n",
    "    \n",
    "    #regressor models\n",
    "\n",
    "    reg = MODELS_stack\n",
    "\n",
    "    reg.fit(x_train,df_train.SalePrice.values)\n",
    "    valid_preds = reg.predict(x_valid)\n",
    "    test_preds = reg.predict(df_test[numeric_features].values)\n",
    "    \n",
    "    \n",
    "    #joblib.dump(features, os.path.join(config.models_location, f\"{model}_{fold}_columns.bin\"))\n",
    "\n",
    "    # scoring\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(df_valid.SalePrice.values, valid_preds))\n",
    "    mae = mean_absolute_error(df_valid.SalePrice.values, valid_preds)\n",
    "    \n",
    "    print(f\"FOLD={fold}, MODEL = {model}, RMSE = {rmse}, MAE ={mae} \")\n",
    "    return(test_preds)\n",
    "\n",
    "\n",
    "for fold in range(3):\n",
    "    preds_df[\"fold\"+str(fold)+\"stack\"] = run(fold,keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      fold0elasticnet     fold0lasso     fold0ridge     fold0gradb  \\\n",
       "0       114403.640786  114172.701971  113653.436570  125472.560813   \n",
       "1       155481.424279  155280.438493  156563.334103  158616.863844   \n",
       "2       177058.378439  177064.572110  179670.012902  190683.837742   \n",
       "3       197831.202756  198193.370135  195844.429916  193106.454321   \n",
       "4       195556.464756  193227.706610  193712.568856  194723.662508   \n",
       "...               ...            ...            ...            ...   \n",
       "1454     87141.131838   86761.804485   86049.771152   80323.552209   \n",
       "1455     79517.919732   79783.412802   79622.892699   78115.888082   \n",
       "1456    164899.234693  166153.653972  167760.061751  151705.591070   \n",
       "1457    112517.019210  112865.114830  112402.212724  119076.802312   \n",
       "1458    220310.072059  221598.362640  217885.304672  209522.068561   \n",
       "\n",
       "           fold0svr   fold0xgboost  fold1elasticnet     fold1lasso  \\\n",
       "0     116187.997511  127085.296875    111087.582845  111087.582845   \n",
       "1     188605.914184  168763.843750    153711.088886  153711.088886   \n",
       "2     188898.104398  191321.453125    179615.861128  179615.861128   \n",
       "3     199404.978834  200636.718750    201762.109390  201762.109390   \n",
       "4     186192.991397  193445.859375    190190.555452  190190.555452   \n",
       "...             ...            ...              ...            ...   \n",
       "1454   89174.729196   78944.281250     88466.306102   88466.306102   \n",
       "1455   82423.854960   78522.140625     83597.834091   83597.834091   \n",
       "1456  176585.627093  168415.109375    175228.977027  175228.977027   \n",
       "1457  116067.165078  117482.390625    116059.433804  116059.433804   \n",
       "1458  220497.245970  216235.750000    226943.193494  226943.193494   \n",
       "\n",
       "         fold1ridge     fold1gradb       fold1svr   fold1xgboost  \\\n",
       "0     109704.871615  119968.787138  112538.308375  121812.109375   \n",
       "1     153119.133427  159580.818147  172586.477317  164025.593750   \n",
       "2     179021.714271  187164.202817  188690.361535  189356.125000   \n",
       "3     201345.337381  190601.749482  201432.637619  195654.296875   \n",
       "4     194492.761572  188413.786936  189492.132130  186901.234375   \n",
       "...             ...            ...            ...            ...   \n",
       "1454   87108.199362   81022.279088   85587.232984   82179.656250   \n",
       "1455   81730.900836   79789.978092   81391.114588   83464.734375   \n",
       "1456  177974.238180  163048.217502  182360.627752  169279.500000   \n",
       "1457  116355.511355  120027.869605  122762.402244  119866.281250   \n",
       "1458  230352.411992  213667.805330  234315.277170  230719.171875   \n",
       "\n",
       "      fold2elasticnet     fold2lasso     fold2ridge     fold2gradb  \\\n",
       "0       111886.273014  111499.982283  113795.261731  129559.604746   \n",
       "1       159164.920221  158924.176879  165242.898224  165169.401095   \n",
       "2       179946.044122  179945.606024  178129.780628  192703.057585   \n",
       "3       201623.004721  201611.168605  200364.054317  192276.842692   \n",
       "4       194795.403016  193499.424791  195394.926042  198679.935585   \n",
       "...               ...            ...            ...            ...   \n",
       "1454     90525.457470   90614.842525   88123.539585   88090.145011   \n",
       "1455     87974.476337   87833.018309   86904.642420   90926.143295   \n",
       "1456    169610.172166  170464.940681  166337.886159  169408.261529   \n",
       "1457    114406.230886  114536.584053  112692.445888  115566.526416   \n",
       "1458    223613.740929  223872.940537  223095.315887  210613.465874   \n",
       "\n",
       "           fold2svr   fold2xgboost     fold0stack     fold1stack  \\\n",
       "0     116075.358142  134144.031250  124069.625000  116233.101562   \n",
       "1     178463.256188  168221.843750  176975.906250  168921.156250   \n",
       "2     190221.539893  204268.562500  196286.562500  184666.859375   \n",
       "3     201094.780724  204251.218750  202652.437500  191000.046875   \n",
       "4     195482.681005  209314.984375  194762.187500  192752.328125   \n",
       "...             ...            ...            ...            ...   \n",
       "1454   89968.824670   88825.953125   83156.218750   85082.296875   \n",
       "1455   88628.121850   96494.437500   79472.679688   92416.929688   \n",
       "1456  188849.224548  174543.312500  163912.859375  177495.843750   \n",
       "1457  121387.181440  118632.820312  123069.468750  121243.390625   \n",
       "1458  220133.438429  234049.843750  217086.421875  229055.812500   \n",
       "\n",
       "         fold2stack      SalePrice  \n",
       "0     135502.593750  120980.533995  \n",
       "1     177016.765625  166977.421546  \n",
       "2     207934.203125  189681.111318  \n",
       "3     216013.343750  199801.358292  \n",
       "4     212811.453125  195964.511947  \n",
       "...             ...            ...  \n",
       "1454   92469.125000   86352.743976  \n",
       "1455   98044.218750   86026.868687  \n",
       "1456  185286.937500  171781.765019  \n",
       "1457  122604.515625  118821.337096  \n",
       "1458  233985.125000  222990.344846  \n",
       "\n",
       "[1459 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fold0elasticnet</th>\n      <th>fold0lasso</th>\n      <th>fold0ridge</th>\n      <th>fold0gradb</th>\n      <th>fold0svr</th>\n      <th>fold0xgboost</th>\n      <th>fold1elasticnet</th>\n      <th>fold1lasso</th>\n      <th>fold1ridge</th>\n      <th>fold1gradb</th>\n      <th>fold1svr</th>\n      <th>fold1xgboost</th>\n      <th>fold2elasticnet</th>\n      <th>fold2lasso</th>\n      <th>fold2ridge</th>\n      <th>fold2gradb</th>\n      <th>fold2svr</th>\n      <th>fold2xgboost</th>\n      <th>fold0stack</th>\n      <th>fold1stack</th>\n      <th>fold2stack</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>114403.640786</td>\n      <td>114172.701971</td>\n      <td>113653.436570</td>\n      <td>125472.560813</td>\n      <td>116187.997511</td>\n      <td>127085.296875</td>\n      <td>111087.582845</td>\n      <td>111087.582845</td>\n      <td>109704.871615</td>\n      <td>119968.787138</td>\n      <td>112538.308375</td>\n      <td>121812.109375</td>\n      <td>111886.273014</td>\n      <td>111499.982283</td>\n      <td>113795.261731</td>\n      <td>129559.604746</td>\n      <td>116075.358142</td>\n      <td>134144.031250</td>\n      <td>124069.625000</td>\n      <td>116233.101562</td>\n      <td>135502.593750</td>\n      <td>120980.533995</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>155481.424279</td>\n      <td>155280.438493</td>\n      <td>156563.334103</td>\n      <td>158616.863844</td>\n      <td>188605.914184</td>\n      <td>168763.843750</td>\n      <td>153711.088886</td>\n      <td>153711.088886</td>\n      <td>153119.133427</td>\n      <td>159580.818147</td>\n      <td>172586.477317</td>\n      <td>164025.593750</td>\n      <td>159164.920221</td>\n      <td>158924.176879</td>\n      <td>165242.898224</td>\n      <td>165169.401095</td>\n      <td>178463.256188</td>\n      <td>168221.843750</td>\n      <td>176975.906250</td>\n      <td>168921.156250</td>\n      <td>177016.765625</td>\n      <td>166977.421546</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>177058.378439</td>\n      <td>177064.572110</td>\n      <td>179670.012902</td>\n      <td>190683.837742</td>\n      <td>188898.104398</td>\n      <td>191321.453125</td>\n      <td>179615.861128</td>\n      <td>179615.861128</td>\n      <td>179021.714271</td>\n      <td>187164.202817</td>\n      <td>188690.361535</td>\n      <td>189356.125000</td>\n      <td>179946.044122</td>\n      <td>179945.606024</td>\n      <td>178129.780628</td>\n      <td>192703.057585</td>\n      <td>190221.539893</td>\n      <td>204268.562500</td>\n      <td>196286.562500</td>\n      <td>184666.859375</td>\n      <td>207934.203125</td>\n      <td>189681.111318</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>197831.202756</td>\n      <td>198193.370135</td>\n      <td>195844.429916</td>\n      <td>193106.454321</td>\n      <td>199404.978834</td>\n      <td>200636.718750</td>\n      <td>201762.109390</td>\n      <td>201762.109390</td>\n      <td>201345.337381</td>\n      <td>190601.749482</td>\n      <td>201432.637619</td>\n      <td>195654.296875</td>\n      <td>201623.004721</td>\n      <td>201611.168605</td>\n      <td>200364.054317</td>\n      <td>192276.842692</td>\n      <td>201094.780724</td>\n      <td>204251.218750</td>\n      <td>202652.437500</td>\n      <td>191000.046875</td>\n      <td>216013.343750</td>\n      <td>199801.358292</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>195556.464756</td>\n      <td>193227.706610</td>\n      <td>193712.568856</td>\n      <td>194723.662508</td>\n      <td>186192.991397</td>\n      <td>193445.859375</td>\n      <td>190190.555452</td>\n      <td>190190.555452</td>\n      <td>194492.761572</td>\n      <td>188413.786936</td>\n      <td>189492.132130</td>\n      <td>186901.234375</td>\n      <td>194795.403016</td>\n      <td>193499.424791</td>\n      <td>195394.926042</td>\n      <td>198679.935585</td>\n      <td>195482.681005</td>\n      <td>209314.984375</td>\n      <td>194762.187500</td>\n      <td>192752.328125</td>\n      <td>212811.453125</td>\n      <td>195964.511947</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>87141.131838</td>\n      <td>86761.804485</td>\n      <td>86049.771152</td>\n      <td>80323.552209</td>\n      <td>89174.729196</td>\n      <td>78944.281250</td>\n      <td>88466.306102</td>\n      <td>88466.306102</td>\n      <td>87108.199362</td>\n      <td>81022.279088</td>\n      <td>85587.232984</td>\n      <td>82179.656250</td>\n      <td>90525.457470</td>\n      <td>90614.842525</td>\n      <td>88123.539585</td>\n      <td>88090.145011</td>\n      <td>89968.824670</td>\n      <td>88825.953125</td>\n      <td>83156.218750</td>\n      <td>85082.296875</td>\n      <td>92469.125000</td>\n      <td>86352.743976</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>79517.919732</td>\n      <td>79783.412802</td>\n      <td>79622.892699</td>\n      <td>78115.888082</td>\n      <td>82423.854960</td>\n      <td>78522.140625</td>\n      <td>83597.834091</td>\n      <td>83597.834091</td>\n      <td>81730.900836</td>\n      <td>79789.978092</td>\n      <td>81391.114588</td>\n      <td>83464.734375</td>\n      <td>87974.476337</td>\n      <td>87833.018309</td>\n      <td>86904.642420</td>\n      <td>90926.143295</td>\n      <td>88628.121850</td>\n      <td>96494.437500</td>\n      <td>79472.679688</td>\n      <td>92416.929688</td>\n      <td>98044.218750</td>\n      <td>86026.868687</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>164899.234693</td>\n      <td>166153.653972</td>\n      <td>167760.061751</td>\n      <td>151705.591070</td>\n      <td>176585.627093</td>\n      <td>168415.109375</td>\n      <td>175228.977027</td>\n      <td>175228.977027</td>\n      <td>177974.238180</td>\n      <td>163048.217502</td>\n      <td>182360.627752</td>\n      <td>169279.500000</td>\n      <td>169610.172166</td>\n      <td>170464.940681</td>\n      <td>166337.886159</td>\n      <td>169408.261529</td>\n      <td>188849.224548</td>\n      <td>174543.312500</td>\n      <td>163912.859375</td>\n      <td>177495.843750</td>\n      <td>185286.937500</td>\n      <td>171781.765019</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>112517.019210</td>\n      <td>112865.114830</td>\n      <td>112402.212724</td>\n      <td>119076.802312</td>\n      <td>116067.165078</td>\n      <td>117482.390625</td>\n      <td>116059.433804</td>\n      <td>116059.433804</td>\n      <td>116355.511355</td>\n      <td>120027.869605</td>\n      <td>122762.402244</td>\n      <td>119866.281250</td>\n      <td>114406.230886</td>\n      <td>114536.584053</td>\n      <td>112692.445888</td>\n      <td>115566.526416</td>\n      <td>121387.181440</td>\n      <td>118632.820312</td>\n      <td>123069.468750</td>\n      <td>121243.390625</td>\n      <td>122604.515625</td>\n      <td>118821.337096</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>220310.072059</td>\n      <td>221598.362640</td>\n      <td>217885.304672</td>\n      <td>209522.068561</td>\n      <td>220497.245970</td>\n      <td>216235.750000</td>\n      <td>226943.193494</td>\n      <td>226943.193494</td>\n      <td>230352.411992</td>\n      <td>213667.805330</td>\n      <td>234315.277170</td>\n      <td>230719.171875</td>\n      <td>223613.740929</td>\n      <td>223872.940537</td>\n      <td>223095.315887</td>\n      <td>210613.465874</td>\n      <td>220133.438429</td>\n      <td>234049.843750</td>\n      <td>217086.421875</td>\n      <td>229055.812500</td>\n      <td>233985.125000</td>\n      <td>222990.344846</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 331
    }
   ],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in preds_df.columns:\n",
    "    preds_df[cols] = np.expm1(preds_df[cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted_average aka blending # adjust if you did not use stacked-regressor. \n",
    "\n",
    "for e,col in enumerate(preds_df.columns):\n",
    "    if e == 0:\n",
    "        preds_df['SalePrice'] = preds_df[col]\n",
    "    elif col in ['fold0stack','fold1stack','fold2stack']:\n",
    "        preds_df['SalePrice'] += preds_df[col]*4\n",
    "    else:\n",
    "        preds_df['SalePrice'] += preds_df[col]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    " preds_df['SalePrice'] =  preds_df['SalePrice']/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/hazim/Desktop/Advanced-house-price-prediction/input/test.csv',usecols=[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df[\"id\"] = df_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc  = preds_df[['id','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abc.to_csv(f\"/home/hazim/Desktop/Advanced-house-price-prediction/output/abc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}